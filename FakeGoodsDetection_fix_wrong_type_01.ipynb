{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FakeGoodsDetection fix wrong type 01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xixKlM4WbTwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61ab51c0-4807-4e54-90fc-ee364b8df673"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install pythainlp\n",
        "!pip install stop_words\n",
        "!pip install deepcut\n",
        "!pip install attacut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=c8c4a9e333a07707aec71a614850a5c1b1343bd602f472cfe4b0207918f56fd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "Collecting pythainlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/94/1ca5c23bfdbc0f27fa26e5eeda47d8ff422cbbd3f38c0b8a160fa17a2583/pythainlp-2.0.7-py3-none-any.whl (11.0MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0MB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.2 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (3.2.5)\n",
            "Collecting marisa-trie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2.21.0)\n",
            "Collecting tinydb\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/83/2d46115b89640e9b85b94df47216547396e94125245dd3ade186036ce976/tinydb-3.15.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pythainlp) (4.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2018.9)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pythainlp) (0.3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.2->pythainlp) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pythainlp) (3.0.4)\n",
            "Building wheels for collected packages: marisa-trie\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp36-cp36m-linux_x86_64.whl size=862199 sha256=4f0c17d41066335c31ee1934409df335b0b61ce3cfd306ee2f29888fafd63cb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "Successfully built marisa-trie\n",
            "Installing collected packages: marisa-trie, tinydb, pythainlp\n",
            "Successfully installed marisa-trie-0.7.5 pythainlp-2.0.7 tinydb-3.15.1\n",
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=222a92bec4dbdc59a5135e15f23f36a5ccacd121cb639a1b1ffc199dcda7bfd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n",
            "Collecting deepcut\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/f3/ecda1d7dc51da0689b2df3d002541d0d04ac4db02c5d148eca48c8e3d219/deepcut-0.7.0.0-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepcut) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepcut) (0.21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepcut) (0.25.3)\n",
            "Collecting tensorflow>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deepcut) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepcut) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->deepcut) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->deepcut) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepcut) (2018.9)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (0.33.6)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 49.7MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (3.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->deepcut) (0.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=2.0.0->deepcut) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (3.1.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (0.2.7)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow>=2.0.0->deepcut) (2.8)\n",
            "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow, deepcut\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed deepcut-0.7.0.0 google-auth-1.7.1 tensorboard-2.0.1 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting attacut\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/06/091491dbb7a73a2cb6ac2ed70476d14502224a126addf25a9ae9799beb04/attacut-1.0.5-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 3.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from attacut) (1.12.0)\n",
            "Collecting nptyping>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/bb/2d8603ddde6a63a4e3a3e1106fb2bfecba7f70ccece047baefb919fa412f/nptyping-0.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from attacut) (1.3.1+cu100)\n",
            "Collecting ssg>=0.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/2b/9cf956a0088f44895d20ab0aa008d8f87cc1b1a210af14601aaf72dec729/ssg-0.0.6-py3-none-any.whl (473kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from attacut) (1.17.4)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from attacut) (0.6.2)\n",
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.4MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 52.5MB/s \n",
            "\u001b[?25hCollecting typish\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c7/dfbf7953820a6644a261d9aa05114affb4c146a008330fe4fbd8f1a627c4/typish-1.2.0-py3-none-any.whl\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 16.1MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.32.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/08/8505f192efc72bfafec79655e1d8351d219e2b80b0dec4ae71f50934c17a/tqdm-4.38.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->attacut) (1.1.0)\n",
            "Building wheels for collected packages: fire, pyyaml\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=496e5bac6b34933617fcdbae84e46e86897dfd2f8f4e7583af3dafe7714a33ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=66997461eabf690dd65c3a4f69a9ec8e5faabbaf839bb978f7427099cc2124b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built fire pyyaml\n",
            "Installing collected packages: typish, nptyping, python-crfsuite, tqdm, fire, ssg, pyyaml, attacut\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed attacut-1.0.5 fire-0.2.1 nptyping-0.3.1 python-crfsuite-0.9.6 pyyaml-5.1.2 ssg-0.0.6 tqdm-4.38.0 typish-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI1NDOgsbleE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4b8187d2-5ebc-42c8-a691-f1b7bc768773"
      },
      "source": [
        "from google.colab import drive\n",
        "import string\n",
        "import re\n",
        "import emoji\n",
        "import pythainlp\n",
        "from pythainlp.tokenize import dict_trie, word_tokenize\n",
        "from pythainlp.corpus import thai_stopwords, thai_words\n",
        "from pythainlp.corpus import wordnet\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import words\n",
        "from stop_words import get_stop_words\n",
        "import nltk\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3nn4O0UPPJw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c877a9a4-841f-4898-a3b3-6f2d2836c674"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anjtkB3QRMPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8792cbdd-994f-47d7-9efa-f3ba5ecd3f05"
      },
      "source": [
        "!ls '/content/gdrive/My Drive/Colab Notebooks/Data Project/data/fix wrong type'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'fix wrong type.gsheet'   test_data.csv       train_data.csv\n",
            " test_data_01.csv\t  train_data_01.csv  'train_data v2.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448R-a-7fSib"
      },
      "source": [
        "**1. Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FSuCD7bfSJR"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "path = '/content/gdrive/My Drive/Colab Notebooks/Data Project/data/fix wrong type/'\n",
        "filename = 'test_data_01.csv'\n",
        "labels, texts, rating_start, comment_time, shopid, itemid = [], [], [], [], [], []\n",
        "with open(path+filename, 'r', newline='') as f:\n",
        "  reader = csv.reader(f)\n",
        "  header = next(reader)\n",
        "  for row in reader:\n",
        "    #print(row[2].strip())\n",
        "    labels.append(row[6])\n",
        "    texts.append(row[7].strip())\n",
        "    rating_start.append(row[25])\n",
        "    comment_time.append(row[8])\n",
        "    shopid.append(row[26])\n",
        "    itemid.append(row[16])\n",
        "test_data = pd.DataFrame()\n",
        "test_data['text'] = texts\n",
        "test_data['label'] = labels\n",
        "test_data['rating_start'] = rating_start\n",
        "test_data['comment_time'] = comment_time\n",
        "test_data['shopid'] = shopid\n",
        "test_data['itemid'] = itemid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QupeAgrZbwZO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "ca2f7300-01f4-4c82-bd1b-c89cd36ab2e0"
      },
      "source": [
        "test_data.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>rating_start</th>\n",
              "      <th>comment_time</th>\n",
              "      <th>shopid</th>\n",
              "      <th>itemid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>เพิ่งลองสั่งครั้งแรกค่ะยังไม่เคยลองใช้ ทางร้าน...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>9/2/2019 18:00</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>สินค้าดี มีคุณภาพ ..สินค้าดี มีคุณภาพ ..สินค้า...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/4/2019 14:54</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ห่อดีมาก พันbubble แน่นมากจนอยากจะหักดาว 55555...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3/30/2019 14:55</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Estee Lauder  Advanced Night Repair Synchroniz...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7/30/2019 12:28</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก ความรวดเ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/9/2018 6:40</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ของแท้แน่นอนค่ะ ทางร้านบริการดีมาก มีของแถมให้...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3/4/2019 10:31</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>pack ของดีมาก มีถ่ายรูปแจ้งก่อนส่งสินค้า ส่งไว...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8/5/2019 12:43</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก ความรวดเ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/9/2018 12:18</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ทางร้านจัดส่งสินค้ารวดเร็วดี สินค้าคุณภาพดี ch...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5/9/2019 13:41</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>สินค้าส่งเร็วค่ะ ทางร้านแจ้งตลอด เข้าไปอ่านจาก...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6/1/2019 1:43</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก  การจัดส...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/15/2018 18:17</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>เคยซื้อแล้ว ร้านนี้ซื้อซ้ำบ่อย ของดีคุณภาพ ราค...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6/15/2019 8:10</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>สินค้าร้านนี้ของแท้แน่นอนซื้อใช้หลายรอบแล้ว แม...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8/31/2019 6:46</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ส่งของไวมากๆค่ะถึงแม้ช่วง sell ขวดมีรอยถลอกบ้า...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12/21/2018 22:24</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/17/2019 10:56</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>แม่ค้ามีความรอบคอบละเอียด มีถ่ายรูป​สินค้าก่อน...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5/28/2019 20:23</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>ก่อนแม่ค้าส่งของ แม่ค้าทักมาแจ้งก่อนด้วย น่ารั...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4/4/2019 12:04</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>‘’คุ้มค่ามากครับpackสินค้ามาดี สินค้าจัดส่งได้...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6/27/2019 21:57</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ส่งเร็ว pack มาดี  คุณภาพของสินค้าดีมาก ความคุ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/24/2018 14:47</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ส่งเร็วมากๆๆๆ มีถ่ายรูปให้ดูก่อนส่งด้วย คือใส่...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/11/2019 22:54</td>\n",
              "      <td>4343306</td>\n",
              "      <td>54485355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text label  ...   shopid    itemid\n",
              "0   เพิ่งลองสั่งครั้งแรกค่ะยังไม่เคยลองใช้ ทางร้าน...     0  ...  4343306  54485355\n",
              "1   สินค้าดี มีคุณภาพ ..สินค้าดี มีคุณภาพ ..สินค้า...     0  ...  4343306  54485355\n",
              "2   ห่อดีมาก พันbubble แน่นมากจนอยากจะหักดาว 55555...     0  ...  4343306  54485355\n",
              "3   Estee Lauder  Advanced Night Repair Synchroniz...     0  ...  4343306  54485355\n",
              "4   คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก ความรวดเ...     0  ...  4343306  54485355\n",
              "5   ของแท้แน่นอนค่ะ ทางร้านบริการดีมาก มีของแถมให้...     0  ...  4343306  54485355\n",
              "6   pack ของดีมาก มีถ่ายรูปแจ้งก่อนส่งสินค้า ส่งไว...     0  ...  4343306  54485355\n",
              "7   คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก ความรวดเ...     0  ...  4343306  54485355\n",
              "8   ทางร้านจัดส่งสินค้ารวดเร็วดี สินค้าคุณภาพดี ch...     0  ...  4343306  54485355\n",
              "9   สินค้าส่งเร็วค่ะ ทางร้านแจ้งตลอด เข้าไปอ่านจาก...     0  ...  4343306  54485355\n",
              "10  คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก  การจัดส...     0  ...  4343306  54485355\n",
              "11  เคยซื้อแล้ว ร้านนี้ซื้อซ้ำบ่อย ของดีคุณภาพ ราค...     0  ...  4343306  54485355\n",
              "12  สินค้าร้านนี้ของแท้แน่นอนซื้อใช้หลายรอบแล้ว แม...     0  ...  4343306  54485355\n",
              "13  ส่งของไวมากๆค่ะถึงแม้ช่วง sell ขวดมีรอยถลอกบ้า...     0  ...  4343306  54485355\n",
              "14  ส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส่งไวส...     0  ...  4343306  54485355\n",
              "15  แม่ค้ามีความรอบคอบละเอียด มีถ่ายรูป​สินค้าก่อน...     0  ...  4343306  54485355\n",
              "16  ก่อนแม่ค้าส่งของ แม่ค้าทักมาแจ้งก่อนด้วย น่ารั...     0  ...  4343306  54485355\n",
              "17  ‘’คุ้มค่ามากครับpackสินค้ามาดี สินค้าจัดส่งได้...     0  ...  4343306  54485355\n",
              "18  ส่งเร็ว pack มาดี  คุณภาพของสินค้าดีมาก ความคุ...     0  ...  4343306  54485355\n",
              "19  ส่งเร็วมากๆๆๆ มีถ่ายรูปให้ดูก่อนส่งด้วย คือใส่...     0  ...  4343306  54485355\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSMmiN_Ttnwb"
      },
      "source": [
        "path = '/content/gdrive/My Drive/Colab Notebooks/Data Project/data/fix wrong type/'\n",
        "filename = 'train_data_01.csv'\n",
        "labels, texts, rating_start, comment_time, shopid, itemid = [], [], [], [], [], []\n",
        "with open(path+filename, 'r', newline='') as f:\n",
        "  reader = csv.reader(f)\n",
        "  header = next(reader)\n",
        "  for row in reader:\n",
        "    #print(row[2].strip())\n",
        "    labels.append(row[6])\n",
        "    texts.append(row[7].strip())\n",
        "    rating_start.append(row[25])\n",
        "    comment_time.append(row[8])\n",
        "    shopid.append(row[26])\n",
        "    itemid.append(row[16])\n",
        "train_data = pd.DataFrame()\n",
        "train_data['text'] = texts\n",
        "train_data['label'] = labels\n",
        "train_data['rating_start'] = rating_start\n",
        "train_data['comment_time'] = comment_time\n",
        "train_data['shopid'] = shopid\n",
        "train_data['itemid'] = itemid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVb4S10Dtxkw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "df5c8249-0035-49b0-fca1-531a66fca292"
      },
      "source": [
        "train_data.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>rating_start</th>\n",
              "      <th>comment_time</th>\n",
              "      <th>shopid</th>\n",
              "      <th>itemid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>สีและเนื้อผลิตภัณฑ์ไม่เหมือนที่ใช้อยู่ค่ะ ตัวท...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11/13/2018 20:53</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>คุ้มค่ากับการรอคอยแท้💯%</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12/7/2018 21:19</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>คุณภาพของสินค้าดี ความคุ้มค่าดีมาก ความรวดเร็ว...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>8/2/2019 20:15</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก ความรวดเ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12/3/2018 19:21</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ตัวสินค้าต่างจากสินค้าจาก shop (สีจางกว่า กลิ่...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12/14/2018 23:23</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ส่งของเร็วคะ  check  code ขึ้น  test กลิ่นผ่าน...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/10/2019 14:27</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>สินค้าpack มาดีมากค่ะ จัดส่งรวดเร็วทันใจ คุณภา...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5/4/2019 7:25</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ความคุ้มค่าดีมาก ความรวดเร็วในการจัดส่งดีมาก ก...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/4/2018 15:59</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>แต่จุกยางเก่านิดหนึ่ง แต่ขอลงแท้แน่นอนค่ะ</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12/24/2018 12:46</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ส่งเร็ว pack ดี ไม่มีตำหนิ ของแท้ ป้ายคิงพาวเว...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/4/2019 21:46</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>หัก1คะแนน รอสินค้านานไปหน่อย</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12/7/2018 7:16</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ขอบอกว่า ข้อดีก่อนนะคะ  - สินค้าเป็นของแท้คะ -...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9/26/2019 11:08</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>สินค้าส่งเร็วมาก คุณภาพดี ห่อได้มิดชิด ประทับใ...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10/4/2019 12:10</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก รอของนาน...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/10/2018 19:29</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ความคุ้มค่าดีมาก</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/11/2018 11:36</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>ดีงามมาก</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>9/14/2019 19:23</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>กล่องป้ายคิงพาวเวอร์ แต่เนื้อครีมไม่มั่นใจ เพร...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>10/6/2019 8:46</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>สินค้าคุณภาพโอเค  pack  ห่อมาดีค่ะ</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3/6/2019 7:26</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ส่งช้าแต่ราคาคุ้มค่ามากๆคะ แม่ค้าตอบ chat ตลอด...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11/11/2018 11:41</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ความรวดเร็วในการจัดส่งแย่</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>11/29/2018 17:55</td>\n",
              "      <td>84219986</td>\n",
              "      <td>1592175112</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  ...      itemid\n",
              "0   สีและเนื้อผลิตภัณฑ์ไม่เหมือนที่ใช้อยู่ค่ะ ตัวท...  ...  1592175112\n",
              "1                             คุ้มค่ากับการรอคอยแท้💯%  ...  1592175112\n",
              "2   คุณภาพของสินค้าดี ความคุ้มค่าดีมาก ความรวดเร็ว...  ...  1592175112\n",
              "3   คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก ความรวดเ...  ...  1592175112\n",
              "4   ตัวสินค้าต่างจากสินค้าจาก shop (สีจางกว่า กลิ่...  ...  1592175112\n",
              "5   ส่งของเร็วคะ  check  code ขึ้น  test กลิ่นผ่าน...  ...  1592175112\n",
              "6   สินค้าpack มาดีมากค่ะ จัดส่งรวดเร็วทันใจ คุณภา...  ...  1592175112\n",
              "7   ความคุ้มค่าดีมาก ความรวดเร็วในการจัดส่งดีมาก ก...  ...  1592175112\n",
              "8           แต่จุกยางเก่านิดหนึ่ง แต่ขอลงแท้แน่นอนค่ะ  ...  1592175112\n",
              "9   ส่งเร็ว pack ดี ไม่มีตำหนิ ของแท้ ป้ายคิงพาวเว...  ...  1592175112\n",
              "10                       หัก1คะแนน รอสินค้านานไปหน่อย  ...  1592175112\n",
              "11  ขอบอกว่า ข้อดีก่อนนะคะ  - สินค้าเป็นของแท้คะ -...  ...  1592175112\n",
              "12  สินค้าส่งเร็วมาก คุณภาพดี ห่อได้มิดชิด ประทับใ...  ...  1592175112\n",
              "13  คุณภาพของสินค้าดีมาก ความคุ้มค่าดีมาก รอของนาน...  ...  1592175112\n",
              "14                                   ความคุ้มค่าดีมาก  ...  1592175112\n",
              "15                                           ดีงามมาก  ...  1592175112\n",
              "16  กล่องป้ายคิงพาวเวอร์ แต่เนื้อครีมไม่มั่นใจ เพร...  ...  1592175112\n",
              "17                 สินค้าคุณภาพโอเค  pack  ห่อมาดีค่ะ  ...  1592175112\n",
              "18  ส่งช้าแต่ราคาคุ้มค่ามากๆคะ แม่ค้าตอบ chat ตลอด...  ...  1592175112\n",
              "19                          ความรวดเร็วในการจัดส่งแย่  ...  1592175112\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKRZOu3tnAf8"
      },
      "source": [
        "**2. Clean Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGv1NioD3Otg"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    return emoji.get_emoji_regexp().sub(u'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm6x6THkmuUA"
      },
      "source": [
        "def clean_text(text):    \n",
        "  text = text.replace(u'\\xa0', u' ')\n",
        "  text = text.replace(u'\\u200b', u' ') \n",
        "  text = text.replace(u'\\U0001f970', u' ') \n",
        "  text = text.replace(u'\\U0001f9f4', u' ') \n",
        "  text = text.replace(u'\\U0001f97a', u' ')\n",
        "  text = remove_emoji(text)\n",
        "  \n",
        "  # ลบ เครื่องหมายคำพูด (punctuation) !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
        "  for c in string.punctuation:\n",
        "    text = re.sub(r'\\{}'.format(c),'',text)\n",
        "  \n",
        "  # delete ‘’‼\n",
        "  for c in '‘’‼“”ๆ…️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️️': \n",
        "    text = re.sub(r'\\{}'.format(c),'',text)\n",
        "    \n",
        "  # ลบ separator เช่น \\n \\t\n",
        "  #msg = ' '.join(msg.split())\n",
        "    \n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXx6CmevlH92"
      },
      "source": [
        "train_data['text'] = train_data['text'].apply(clean_text)\n",
        "test_data['text'] = test_data['text'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJmnCQFTnS3w"
      },
      "source": [
        "**3. Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3td0_XPAT6E-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dabef502-907f-4e7b-fa99-48673bbe5992"
      },
      "source": [
        "nltk.download('words')\n",
        "th_stop = tuple(thai_stopwords())\n",
        "en_stop = tuple(get_stop_words('en'))\n",
        "p_stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EURtKIFhXAfE"
      },
      "source": [
        "# custom_vocab = [\n",
        "#     \"สก้อตเทป\",\n",
        "#     \"แพค\",\n",
        "#     \"เซรั่ม\",\n",
        "#     \"สิ้นค้า\",\n",
        "#     \"คอนวี่\",\n",
        "#     \"เคาเตอร์\",\n",
        "#     \"ทรีทเม้นท์\",\n",
        "#     \"แฟลตเซล\",\n",
        "#     \"สิสค้า\",\n",
        "#     \"เคาร์เตอร์\",\n",
        "#     \"เทส\",\n",
        "#     \"หนีด\",\n",
        "#     \"อีฟแอนด์บอย\",\n",
        "#     \"รับของ\",\n",
        "#     \"ไม่รุ้\",\n",
        "#     \"เคาท์เตอร์\",\n",
        "#     \"เคาว์เตอร์\",\n",
        "#     \"เคาทเตอร์\",\n",
        "#     \"เค้าเตอร์\"\n",
        "# ]\n",
        "# trie = dict_trie(dict_source=list(thai_words()) + custom_vocab)\n",
        "stop = [word for word in thai_stopwords()]\n",
        "stop.remove('ส่ง')\n",
        "stop.append('ล่ะ')\n",
        "stop.append('อะ')\n",
        "stop.append('อะนะ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB6ytrPUxaRf"
      },
      "source": [
        "def split_word(text,engine='newmm'):\n",
        "    \n",
        "    tokens = word_tokenize(text, engine=engine, keep_whitespace=False)\n",
        "    \n",
        "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\n",
        "    tokens = [i for i in tokens if not i in stop]\n",
        "\n",
        "    tokens = [i.lower() for i in tokens]\n",
        "    \n",
        "    # หารากศัพท์ภาษาไทย และภาษาอังกฤษ\n",
        "    # English\n",
        "    #tokens = [p_stemmer.stem(i) for i in tokens]\n",
        "    \n",
        "    # Thai\n",
        "    # tokens_temp=[]\n",
        "    # for i in tokens:\n",
        "    #     w_syn = wordnet.synsets(i)\n",
        "    #     if (len(w_syn)>0) and (len(w_syn[0].lemma_names('tha'))>0):\n",
        "    #         tokens_temp.append(w_syn[0].lemma_names('tha')[0])\n",
        "    #     else:\n",
        "    #         tokens_temp.append(i)\n",
        "    \n",
        "    # tokens = tokens_temp\n",
        "    \n",
        "    # ลบตัวเลข\n",
        "    tokens = [i for i in tokens if not i.isnumeric()]\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h59i7xry8o0"
      },
      "source": [
        "def split_word_newmm(text):\n",
        "    \n",
        "    tokens = word_tokenize(text, engine='newmm', keep_whitespace=False)\n",
        "    \n",
        "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\n",
        "    tokens = [i for i in tokens if not i in stop]\n",
        "\n",
        "    tokens = [i.lower() for i in tokens]\n",
        "    \n",
        "    # ลบตัวเลข\n",
        "    tokens = [i for i in tokens if not i.isnumeric()]\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPwn-HpazE9t"
      },
      "source": [
        "def split_word_deepcut(text):\n",
        "    \n",
        "    tokens = word_tokenize(text, engine='deepcut', keep_whitespace=False)\n",
        "    \n",
        "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\n",
        "    tokens = [i for i in tokens if not i in stop]\n",
        "\n",
        "    tokens = [i.lower() for i in tokens]\n",
        "    \n",
        "    # ลบตัวเลข\n",
        "    tokens = [i for i in tokens if not i.isnumeric()]\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryt18B22zJ5D"
      },
      "source": [
        "def split_word_attacut(text):\n",
        "    \n",
        "    tokens = word_tokenize(text, engine='attacut', keep_whitespace=False)\n",
        "    \n",
        "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\n",
        "    tokens = [i for i in tokens if not i in stop]\n",
        "\n",
        "    tokens = [i.lower() for i in tokens]\n",
        "    \n",
        "    # ลบตัวเลข\n",
        "    tokens = [i for i in tokens if not i.isnumeric()]\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6R-k8kDqeX0"
      },
      "source": [
        "# train_data['newmm'] = train_data['text'].apply(split_word)\n",
        "# train_data['deepcut'] = train_data['text'].apply(split_word,engine='deepcut')\n",
        "# train_data['attacut'] = train_data['text'].apply(split_word,engine='attacut')\n",
        "# test_data['newmm'] = test_data['text'].apply(split_word)\n",
        "# test_data['deepcut'] = test_data['text'].apply(split_word,engine='deepcut')\n",
        "# test_data['attacut'] = test_data['text'].apply(split_word,engine='attacut')\n",
        "# train_df['newmm'] = train_df['text'].apply(word_tokenize, keep_whitespace=False, custom_dict=trie)\n",
        "# train_df['deepcut'] = train_df['text'].apply(word_tokenize,engine='deepcut', keep_whitespace=False)\n",
        "# train_df['attacut'] = train_df['text'].apply(word_tokenize,engine='attacut', keep_whitespace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VvtdwOVFAU_"
      },
      "source": [
        "# for text, label, newmm in review_df.values:\n",
        "#   print(text)\n",
        "#   print(newmm, end='\\n\\n')\n",
        "  #print(fix_wrong_type(newmm, correct_string), end='\\n\\n')\n",
        "  #print(deepcut)\n",
        "  #print(fix_wrong_type(deepcut, correct_string))\n",
        "  #print(attacut)\n",
        "  #print(fix_wrong_type(attacut, correct_string), end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiIYbl5Fcliu"
      },
      "source": [
        "X_train = train_data['text']\n",
        "X_test = test_data['text']\n",
        "y_train = train_data['label']\n",
        "y_test = test_data['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSlu-hSOmiLI"
      },
      "source": [
        "**Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJT24hg2mj2e"
      },
      "source": [
        "**CountVectorizer as features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZriCDODVCPCd"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K86IeI32jVQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f33ab008-d8a5-4586-9a27-45ea4c239938"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer_newmm = CountVectorizer(analyzer=split_word_newmm)\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train_newmm = count_vectorizer_newmm.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test_newmm = count_vectorizer_newmm.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer_newmm.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['advance', 'advanced', 'age', 'air', 'amazing', 'and', 'anr', 'ant', 'app', 'april']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0nhX6kYCSeH"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "510wgLSPCT6L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88cbc9c1-dfc8-40e6-ce5d-c403b5d6f298"
      },
      "source": [
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer_deepcut = CountVectorizer(analyzer=split_word_deepcut)\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train_deepcut = count_vectorizer_deepcut.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test_deepcut = count_vectorizer_deepcut.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer_deepcut.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1ดาว', '30ml', '50ml', '5ดาว', 'advance', 'advance night', 'advanced', 'age', 'air', 'amazing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX67pVlqCkdU"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3pxS2ksCWK_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e7623ee-9832-4041-96fb-51d9c6666096"
      },
      "source": [
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer_attacut = CountVectorizer(analyzer=split_word_attacut)\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train_attacut = count_vectorizer_attacut.fit_transform(X_train.values)\n",
        "/\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test_attacut = count_vectorizer_attacut.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer_attacut.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['advance', 'advanced', 'age', 'air', 'amazing', 'and', 'anr', 'ant', 'app', 'april']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVt_JA9Wyl8B"
      },
      "source": [
        "**TF-IDF Vectors as features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe9TBFjcC5VX"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymr3pNWMyboC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4b9b9315-f86e-4fa5-9b42-5c7551d65274"
      },
      "source": [
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer_newmm = TfidfVectorizer(analyzer=split_word_newmm, max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train_newmm = tfidf_vectorizer_newmm.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test_newmm = tfidf_vectorizer_newmm.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer_newmm.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train_newmm.A[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['advance', 'advanced', 'age', 'air', 'amazing', 'and', 'anr', 'ant', 'app', 'april']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NXfyoSiDAS4"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_owsCv6Aybgf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "24245c61-acc5-4d1b-966d-7c7c86eac620"
      },
      "source": [
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer_deepcut = TfidfVectorizer(analyzer=split_word_deepcut, max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train_deepcut = tfidf_vectorizer_deepcut.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test_deepcut = tfidf_vectorizer_deepcut.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer_deepcut.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train_deepcut.A[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1ดาว', '30ml', '50ml', '5ดาว', 'advance', 'advance night', 'advanced', 'age', 'air', 'amazing']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BD-Jio9DMvz"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVYUiF20DO7M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2cb76155-c4fe-4951-8bd7-be22b142cc5f"
      },
      "source": [
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer_attacut = TfidfVectorizer(analyzer=split_word_attacut, max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train_attacut = tfidf_vectorizer_attacut.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test_attacut = tfidf_vectorizer_attacut.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer_attacut.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train_attacut.A[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['advance', 'advanced', 'age', 'air', 'amazing', 'and', 'anr', 'ant', 'app', 'april']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCgbJECz0U22"
      },
      "source": [
        "**Inspecting the vectors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd3YLdd9FraV"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXa-S-MlybS9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8b440920-7477-4832-81f6-0d30b320863a"
      },
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df_newmm = pd.DataFrame(count_train_newmm.A, columns=count_vectorizer_newmm.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df_newmm = pd.DataFrame(tfidf_train_newmm.A, columns=tfidf_vectorizer_newmm.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df_newmm.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df_newmm.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   advance  advanced  age  air  amazing  and  anr  ...  ไหม  ไหล  ไหว  ไอ  ๅๅๅ  ้  ์\n",
            "0        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "1        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "2        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "3        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "4        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "\n",
            "[5 rows x 1628 columns]\n",
            "   advance  advanced  age  air  amazing  and  ...  ไหล  ไหว   ไอ  ๅๅๅ    ้    ์\n",
            "0      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[5 rows x 1628 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7padGh0Gz2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ca6e0159-9fe8-4ba1-a4e1-be27013befd4"
      },
      "source": [
        "# Calculate the difference in columns: difference\n",
        "difference = set(tfidf_df_newmm.columns) - set(count_df_newmm.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df_newmm.equals(tfidf_df_newmm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set()\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W314G1e4Fu83"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL633rslyhmA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c06a1e9f-ea58-4c3e-917c-b5ded5620d69"
      },
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df_deepcut = pd.DataFrame(count_train_deepcut.A, columns=count_vectorizer_deepcut.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df_deepcut = pd.DataFrame(tfidf_train_deepcut.A, columns=tfidf_vectorizer_deepcut.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df_deepcut.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df_deepcut.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   1ดาว  30ml  50ml  5ดาว  advance  ...  ไหม  ไหร่  ไหล  ไหว  ไอเท็มฮ็อตฮิต\n",
            "0     0     0     0     0        0  ...    0     0    0    0              0\n",
            "1     0     0     0     0        0  ...    0     0    0    0              0\n",
            "2     0     0     0     0        0  ...    0     0    0    0              0\n",
            "3     0     0     0     0        0  ...    0     0    0    0              0\n",
            "4     0     0     0     0        0  ...    0     0    0    0              0\n",
            "\n",
            "[5 rows x 1450 columns]\n",
            "   1ดาว  30ml  50ml  5ดาว  advance  ...  ไหม  ไหร่  ไหล  ไหว  ไอเท็มฮ็อตฮิต\n",
            "0   0.0   0.0   0.0   0.0      0.0  ...  0.0   0.0  0.0  0.0            0.0\n",
            "1   0.0   0.0   0.0   0.0      0.0  ...  0.0   0.0  0.0  0.0            0.0\n",
            "2   0.0   0.0   0.0   0.0      0.0  ...  0.0   0.0  0.0  0.0            0.0\n",
            "3   0.0   0.0   0.0   0.0      0.0  ...  0.0   0.0  0.0  0.0            0.0\n",
            "4   0.0   0.0   0.0   0.0      0.0  ...  0.0   0.0  0.0  0.0            0.0\n",
            "\n",
            "[5 rows x 1449 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2eMpuyfypUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccda310f-5caf-4ea0-ddc9-ccf25d888fa9"
      },
      "source": [
        "# Calculate the difference in columns: difference\n",
        "difference = set(tfidf_df_deepcut.columns) - set(count_df_deepcut.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df_deepcut.equals(tfidf_df_deepcut))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set()\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPlh_75lHQN2"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_S1j9HYyrrM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d1153da7-962c-4a7e-fc76-6975e37bd138"
      },
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df_attacut = pd.DataFrame(count_train_attacut.A, columns=count_vectorizer_attacut.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df_attacut = pd.DataFrame(tfidf_train_attacut.A, columns=tfidf_vectorizer_attacut.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df_attacut.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df_attacut.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   advance  advanced  age  air  amazing  and  anr  ...  ไหม  ไหล  ไหว  ไอ  ๅๅๅ  ้  ์\n",
            "0        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "1        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "2        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "3        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "4        0         0    0    0        0    0    0  ...    0    0    0   0    0  0  0\n",
            "\n",
            "[5 rows x 1628 columns]\n",
            "   advance  advanced  age  air  amazing  and  ...  ไหล  ไหว   ไอ  ๅๅๅ    ้    ์\n",
            "0      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4      0.0       0.0  0.0  0.0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[5 rows x 1628 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zKjhbn9Hg6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a0c4618-8b93-4d22-b13c-1b21626668a6"
      },
      "source": [
        "# Calculate the difference in columns: difference\n",
        "difference = set(tfidf_df_attacut.columns) - set(count_df_attacut.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df_attacut.equals(tfidf_df_attacut))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set()\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBgDqAIG51ZK"
      },
      "source": [
        "**Training and testing model with CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_kxLEnpPleu"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn import decomposition, ensemble\n",
        "import xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usp3wEq7LZsw"
      },
      "source": [
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    pred = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    # Calculate the accuracy score: score\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    print(score)\n",
        "\n",
        "    report = metrics.classification_report(y_test, pred)\n",
        "    print(report)\n",
        "\n",
        "    # Calculate the confusion matrix: cm\n",
        "    cm = metrics.confusion_matrix(y_test, pred, labels=['0', '1'])\n",
        "    print(cm)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9niDfygkN8ut"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F8DguUnN-Yk"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiSK-bmLPsgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "02b29708-d979-4d63-d895-c1b2e787f735"
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "train_model(naive_bayes.MultinomialNB(), count_train_newmm, y_train, count_test_newmm)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "train_model(naive_bayes.MultinomialNB(), tfidf_train_newmm, y_train, tfidf_test_newmm)\n",
        "\n",
        "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
        "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "# print (\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# # Naive Bayes on Character Level TF IDF Vectors\n",
        "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "# print (\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9769911504424779\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99      1108\n",
            "           1       0.44      0.73      0.55        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.72      0.85      0.77      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1088   20]\n",
            " [   6   16]]\n",
            "0.9805309734513274\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.00      0.00      0.00        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.49      0.50      0.50      1130\n",
            "weighted avg       0.96      0.98      0.97      1130\n",
            "\n",
            "[[1108    0]\n",
            " [  22    0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UgHwcVFOEJ7"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbgAUL8OREpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "273060a1-fb9f-479e-e5ed-37ea9b1b5432"
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "train_model(naive_bayes.MultinomialNB(), count_train_deepcut, y_train, count_test_deepcut)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "train_model(naive_bayes.MultinomialNB(), tfidf_train_deepcut, y_train, tfidf_test_deepcut)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9761061946902655\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99      1108\n",
            "           1       0.43      0.68      0.53        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.71      0.83      0.76      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1088   20]\n",
            " [   7   15]]\n",
            "0.9805309734513274\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.00      0.00      0.00        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.49      0.50      0.50      1130\n",
            "weighted avg       0.96      0.98      0.97      1130\n",
            "\n",
            "[[1108    0]\n",
            " [  22    0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyZJZvotORXr"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxkqUq_ERiR6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "97a80aee-18c8-4715-cf16-0e89d6724818"
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "train_model(naive_bayes.MultinomialNB(), count_train_attacut, y_train, count_test_attacut)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "train_model(naive_bayes.MultinomialNB(), tfidf_train_attacut, y_train, tfidf_test_attacut)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9769911504424779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99      1108\n",
            "           1       0.44      0.73      0.55        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.72      0.85      0.77      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1088   20]\n",
            " [   6   16]]\n",
            "0.9805309734513274\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.00      0.00      0.00        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.49      0.50      0.50      1130\n",
            "weighted avg       0.96      0.98      0.97      1130\n",
            "\n",
            "[[1108    0]\n",
            " [  22    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-52p951xJBif"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pw-ri9TXR8H"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpemItnBG2z1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "a147006d-db6c-4a13-f582-56db9e5d5edf"
      },
      "source": [
        "# Logistic Regression on Count Vectors\n",
        "train_model(linear_model.LogisticRegression(), count_train_newmm, y_train, count_test_newmm)\n",
        "\n",
        "# Logistic Regression on Word Level TF IDF Vectors\n",
        "train_model(linear_model.LogisticRegression(), tfidf_train_newmm, y_train, tfidf_test_newmm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9858407079646018\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1108\n",
            "           1       0.67      0.55      0.60        22\n",
            "\n",
            "    accuracy                           0.99      1130\n",
            "   macro avg       0.83      0.77      0.80      1130\n",
            "weighted avg       0.98      0.99      0.99      1130\n",
            "\n",
            "[[1102    6]\n",
            " [  10   12]]\n",
            "0.9823008849557522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       1.00      0.09      0.17        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.99      0.55      0.58      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1108    0]\n",
            " [  20    2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjSpkQJAY2Zf"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8dXwtT3G2yK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "3e6b2d67-3fa7-4862-e9dc-cfb83aa8c345"
      },
      "source": [
        "# Logistic Regression on Count Vectors\n",
        "train_model(linear_model.LogisticRegression(), count_train_deepcut, y_train, count_test_deepcut)\n",
        "\n",
        "# Logistic Regression on Word Level TF IDF Vectors\n",
        "train_model(linear_model.LogisticRegression(), tfidf_train_deepcut, y_train, tfidf_test_deepcut)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9814159292035398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1108\n",
            "           1       0.53      0.36      0.43        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.76      0.68      0.71      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1101    7]\n",
            " [  14    8]]\n",
            "0.9805309734513274\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.50      0.05      0.08        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.74      0.52      0.54      1130\n",
            "weighted avg       0.97      0.98      0.97      1130\n",
            "\n",
            "[[1107    1]\n",
            " [  21    1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEBh5CpFY-KN"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN7TcvTpG2ui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "cd9853b2-28a1-4f37-cf98-ebdcf11d8100"
      },
      "source": [
        "# Logistic Regression on Count Vectors\n",
        "train_model(linear_model.LogisticRegression(), count_train_attacut, y_train, count_test_attacut)\n",
        "\n",
        "# Logistic Regression on Word Level TF IDF Vectors\n",
        "train_model(linear_model.LogisticRegression(), tfidf_train_attacut, y_train, tfidf_test_attacut)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9858407079646018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1108\n",
            "           1       0.67      0.55      0.60        22\n",
            "\n",
            "    accuracy                           0.99      1130\n",
            "   macro avg       0.83      0.77      0.80      1130\n",
            "weighted avg       0.98      0.99      0.99      1130\n",
            "\n",
            "[[1102    6]\n",
            " [  10   12]]\n",
            "0.9823008849557522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       1.00      0.09      0.17        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.99      0.55      0.58      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1108    0]\n",
            " [  20    2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7rYg24ZZn4h"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PXveqNWZ3Gf"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p55hPblOG2mo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "7f6b6287-402f-443e-de45-9ecb42d8e4ad"
      },
      "source": [
        "# Random Forest on Count Vectors\n",
        "train_model(ensemble.RandomForestClassifier(), count_train_newmm, y_train, count_test_newmm)\n",
        "\n",
        "# Random Forest on Word Level TF IDF Vectors\n",
        "train_model(ensemble.RandomForestClassifier(), tfidf_train_newmm, y_train, tfidf_test_newmm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9831858407079646\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.71      0.23      0.34        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.85      0.61      0.67      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1106    2]\n",
            " [  17    5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9814159292035398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.57      0.18      0.28        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.78      0.59      0.63      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1105    3]\n",
            " [  18    4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi4X8DrxaTol"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBIF48W5G2bX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "7f9a24e8-628a-4166-98f4-da9d19bd1240"
      },
      "source": [
        "# Random Forest on Count Vectors\n",
        "train_model(ensemble.RandomForestClassifier(), count_train_deepcut, y_train, count_test_deepcut)\n",
        "\n",
        "# Random Forest on Word Level TF IDF Vectors\n",
        "train_model(ensemble.RandomForestClassifier(), tfidf_train_deepcut, y_train, tfidf_test_deepcut)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9858407079646018\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1108\n",
            "           1       0.88      0.32      0.47        22\n",
            "\n",
            "    accuracy                           0.99      1130\n",
            "   macro avg       0.93      0.66      0.73      1130\n",
            "weighted avg       0.98      0.99      0.98      1130\n",
            "\n",
            "[[1107    1]\n",
            " [  15    7]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9831858407079646\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.71      0.23      0.34        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.85      0.61      0.67      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1106    2]\n",
            " [  17    5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzt-mLd_aZPD"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpP_pplUaar0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "e1dc92d4-df20-4ef4-fece-b72ad10c96d6"
      },
      "source": [
        "# Random Forest on Count Vectors\n",
        "train_model(ensemble.RandomForestClassifier(), count_train_attacut, y_train, count_test_attacut)\n",
        "\n",
        "# Random Forest on Word Level TF IDF Vectors\n",
        "train_model(ensemble.RandomForestClassifier(), tfidf_train_attacut, y_train, tfidf_test_attacut)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9814159292035398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.57      0.18      0.28        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.78      0.59      0.63      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1105    3]\n",
            " [  18    4]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9823008849557522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.67      0.18      0.29        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.83      0.59      0.64      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1106    2]\n",
            " [  18    4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfcCvXmfbeSD"
      },
      "source": [
        "Boosting Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES6f5ZsSbfqz"
      },
      "source": [
        "newmm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxfvPZdyabmU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "e561d203-7e43-4791-c12a-34174523a4f2"
      },
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "train_model(xgboost.XGBClassifier(), count_train_newmm.tocsc(), y_train, count_test_newmm.tocsc())\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "train_model(xgboost.XGBClassifier(), tfidf_train_newmm.tocsc(), y_train, tfidf_test_newmm.tocsc())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9823008849557522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1108\n",
            "           1       0.60      0.27      0.37        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.79      0.63      0.68      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1104    4]\n",
            " [  16    6]]\n",
            "0.9814159292035398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.56      0.23      0.32        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.77      0.61      0.66      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1104    4]\n",
            " [  17    5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HUbtD8cl9Y"
      },
      "source": [
        "deepcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV9yp_s2abgi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "1b28bb45-3181-4c05-d591-62048603f5a1"
      },
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "train_model(xgboost.XGBClassifier(), count_train_deepcut.tocsc(), y_train, count_test_deepcut.tocsc())\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "train_model(xgboost.XGBClassifier(), tfidf_train_deepcut.tocsc(), y_train, tfidf_test_deepcut.tocsc())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9814159292035398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.57      0.18      0.28        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.78      0.59      0.63      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1105    3]\n",
            " [  18    4]]\n",
            "0.9823008849557522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.62      0.23      0.33        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.80      0.61      0.66      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1105    3]\n",
            " [  17    5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPWgN3AZdDSI"
      },
      "source": [
        "attacut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5sy0_BIdEgb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6dfe7ce7-03c1-4469-ea1e-720d122e398c"
      },
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "train_model(xgboost.XGBClassifier(), count_train_attacut.tocsc(), y_train, count_test_attacut.tocsc())\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "train_model(xgboost.XGBClassifier(), tfidf_train_attacut.tocsc(), y_train, tfidf_test_attacut.tocsc())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9823008849557522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1108\n",
            "           1       0.60      0.27      0.37        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.79      0.63      0.68      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1104    4]\n",
            " [  16    6]]\n",
            "0.9814159292035398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1108\n",
            "           1       0.56      0.23      0.32        22\n",
            "\n",
            "    accuracy                           0.98      1130\n",
            "   macro avg       0.77      0.61      0.66      1130\n",
            "weighted avg       0.98      0.98      0.98      1130\n",
            "\n",
            "[[1104    4]\n",
            " [  17    5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwuB5Xc89vD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdFwvn_l89lY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "39543129-faa5-4d12-f04e-5ddb14618536"
      },
      "source": [
        "pred = train_model(linear_model.LogisticRegression(), count_train_newmm, y_train, count_test_newmm)\n",
        "pred_df = pd.DataFrame(pred, columns=['pred'])\n",
        "pred_test_data = pd.concat([test_data, pred_df],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9858407079646018\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1108\n",
            "           1       0.67      0.55      0.60        22\n",
            "\n",
            "    accuracy                           0.99      1130\n",
            "   macro avg       0.83      0.77      0.80      1130\n",
            "weighted avg       0.98      0.99      0.99      1130\n",
            "\n",
            "[[1102    6]\n",
            " [  10   12]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vUXBlmk89fC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d5edf8e2-afa7-49c4-e0d1-250412ed37a8"
      },
      "source": [
        "for shopid in pred_test_data.shopid.unique():\n",
        "  #print(shopid)\n",
        "  df = pred_test_data[pred_test_data['shopid'] == shopid]\n",
        "  pred_count = df.pred.value_counts()\n",
        "  actual_count = df.label.value_counts()\n",
        "  print('shopid: ',shopid,',0:', pred_count[0],',1:',pred_count[1])\n",
        "  print('predict propability: ',round((pred_count[1]+1)/(pred_count[0]+pred_count[1]+2), 2) )\n",
        "  print('actual propability: ',round((actual_count[1]+1)/(actual_count[0]+actual_count[1]+2), 2),'\\n' )\n",
        "\n",
        "\n",
        "  #edit wrong typing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shopid:  4343306 ,0: 254 ,1: 1\n",
            "predict propability:  0.01\n",
            "actual propability:  0.02 \n",
            "\n",
            "shopid:  1514953 ,0: 326 ,1: 1\n",
            "predict propability:  0.01\n",
            "actual propability:  0.01 \n",
            "\n",
            "shopid:  773245 ,0: 224 ,1: 3\n",
            "predict propability:  0.02\n",
            "actual propability:  0.01 \n",
            "\n",
            "shopid:  50796065 ,0: 301 ,1: 9\n",
            "predict propability:  0.03\n",
            "actual propability:  0.03 \n",
            "\n",
            "shopid:  83258020 ,0: 7 ,1: 4\n",
            "predict propability:  0.38\n",
            "actual propability:  0.46 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZOt6LO_IWFg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMyS3OFLIV5Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXLH3MHuabYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3lXn0MXnsJs"
      },
      "source": [
        "def show_classfeats(df, vectorizer, analyzer, score_name='', text_col='texts', class_col='category', is_emoji=False):\n",
        "    vec = vectorizer(analyzer=analyzer)\n",
        "    mat = vec.fit_transform(df[text_col])\n",
        "    dfs = top_feats_by_class(mat, df[class_col], vec.get_feature_names())\n",
        "\n",
        "    if is_emoji:\n",
        "        for dataframe in dfs:\n",
        "            dataframe.columns = [dataframe.label, 'tfidf_' + dataframe.label]\n",
        "        return pd.concat(dfs, axis=1)\n",
        "    else:\n",
        "        #plot_classfeats_h(dfs, score_name=score_name)\n",
        "        print(dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVg0dLWCPiCT"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import string\n",
        "import emoji\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F3Y6IrPSR9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "887bedd8-7114-43b2-c45d-15ca702c6ba3"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "!wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
        "!cp thsarabunnew-webfont.ttf /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/\n",
        "!cp thsarabunnew-webfont.ttf /usr/share/fonts/truetype/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-14 07:50:30--  https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf [following]\n",
            "--2019-11-14 07:50:31--  https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98308 (96K) [application/octet-stream]\n",
            "Saving to: ‘thsarabunnew-webfont.ttf’\n",
            "\n",
            "thsarabunnew-webfon 100%[===================>]  96.00K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-14 07:50:32 (2.48 MB/s) - ‘thsarabunnew-webfont.ttf’ saved [98308/98308]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xys15e_aFyW"
      },
      "source": [
        "matplotlib.font_manager._rebuild()\n",
        "matplotlib.rc('font', family='TH Sarabun New')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iK0Df8iPV9q"
      },
      "source": [
        "def plot_classfeats_h(dfs, score_name=''):\n",
        "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
        "    x = np.arange(len(dfs[0]))\n",
        "\n",
        "    for i, df in enumerate(dfs):\n",
        "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
        "        # ax.spines[\"top\"].set_visible(False)\n",
        "        # ax.spines[\"right\"].set_visible(False)\n",
        "        # ax.set_frame_on(False)\n",
        "        # ax.get_xaxis().tick_bottom()\n",
        "        # ax.get_yaxis().tick_left()\n",
        "        ax.set_xlabel(f\"Mean {score_name} Score\", labelpad=16, fontsize=14)\n",
        "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
        "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
        "        ax.barh(x, df.score, align='center', color='#3F5D7D')\n",
        "        ax.set_yticks(x)\n",
        "        ax.set_ylim([-1, x[-1]+1])\n",
        "        ax.invert_yaxis()\n",
        "        yticks = ax.set_yticklabels(df.feature)\n",
        "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
        "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
        "        calculated across documents with the same class label. '''\n",
        "\n",
        "    dfs = []\n",
        "    labels = np.unique(y)\n",
        "\n",
        "    for label in labels:\n",
        "        ids = np.where(y==label)\n",
        "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
        "        feats_df.label = label\n",
        "        dfs.append(feats_df)\n",
        "\n",
        "    return dfs\n",
        "\n",
        "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
        "    ''' Return the top n features that on average are most important amongst documents in rows\n",
        "        indentified by indices in grp_ids. '''\n",
        "\n",
        "    if grp_ids:\n",
        "        D = Xtr[grp_ids].toarray()\n",
        "    else:\n",
        "        D = Xtr.toarray()\n",
        "\n",
        "    D[D < min_tfidf] = 0\n",
        "    tfidf_means = np.mean(D, axis=0)\n",
        "\n",
        "    return top_feats(tfidf_means, features, top_n)\n",
        "\n",
        "def top_feats(row, features, top_n=25):\n",
        "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
        "\n",
        "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
        "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
        "    df = pd.DataFrame(top_feats)\n",
        "    df.columns = [\"feature\", \"score\"]\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ghkyMpEp4z"
      },
      "source": [
        "def show_classfeats(df, vectorizer, analyzer, score_name='', text_col='texts', class_col='category', is_emoji=False):\n",
        "    vec = vectorizer(analyzer=analyzer)\n",
        "    mat = vec.fit_transform(df[text_col])\n",
        "    dfs = top_feats_by_class(mat, df[class_col], vec.get_feature_names())\n",
        "\n",
        "    if is_emoji:\n",
        "        for dataframe in dfs:\n",
        "            dataframe.columns = [dataframe.label, 'tfidf_' + dataframe.label]\n",
        "        return pd.concat(dfs, axis=1)\n",
        "    else:\n",
        "        #plot_classfeats_h(dfs, score_name=score_name)\n",
        "        print(dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-SKof2kQdS2"
      },
      "source": [
        "def process_text(text):\n",
        "    tokens = split_word_newmm(text) \n",
        "    # stop = tuple(thai_stopwords())\n",
        "    # tokens = word_tokenize(text, engine='attacut', keep_whitespace=False)\n",
        "    \n",
        "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\n",
        "    # tokens = [i for i in tokens if not i in th_stop and not i in en_stop]\n",
        "\n",
        "    # nopunc = [char for char in text if char not in string.punctuation]\n",
        "    # nopunc = \"\".join(nopunc)\n",
        "    \n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXreMEj7OfzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "312b334d-d650-443c-d526-4ae7e40a51b1"
      },
      "source": [
        "# newmm with custom dict\n",
        "show_classfeats(\n",
        "    df=train_data,\n",
        "    vectorizer=TfidfVectorizer,\n",
        "    analyzer=process_text,\n",
        "    score_name=\"tf-idf\",\n",
        "    text_col='text',\n",
        "    class_col='label'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[        feature     score\n",
            "0         ดีมาก  0.192835\n",
            "1        สินค้า  0.097413\n",
            "2        คุณภาพ  0.084594\n",
            "3     การจัดส่ง  0.080464\n",
            "4            ดี  0.076515\n",
            "5   ความคุ้มค่า  0.070219\n",
            "6           ส่ง  0.054677\n",
            "7            กก  0.051286\n",
            "8            ไว  0.039767\n",
            "9     ให้บริการ  0.038172\n",
            "10       ของแท้  0.033707\n",
            "11         สั่ง  0.028724\n",
            "12      ร้านค้า  0.027748\n",
            "13         ราคา  0.027622\n",
            "14         ร้าน  0.025964\n",
            "15       จัดส่ง  0.024829\n",
            "16         ซื้อ  0.023664\n",
            "17          ห่อ  0.023058\n",
            "18         แพ็ค  0.023048\n",
            "19       แน่นอน  0.022216\n",
            "20           แพ  0.019840\n",
            "21        ขนส่ง  0.019673\n",
            "22            ค  0.019195\n",
            "23          ขวด  0.018760\n",
            "24         นะคะ  0.018221,       feature     score\n",
            "0       เนื้อ  0.112539\n",
            "1       กลิ่น  0.111013\n",
            "2        เหลว  0.093340\n",
            "3      เหมือน  0.062123\n",
            "4        แปลก  0.060764\n",
            "5         ขวด  0.059770\n",
            "6        รั่ม  0.057573\n",
            "7          เซ  0.055627\n",
            "8          สี  0.046749\n",
            "9   เหมือนกัน  0.037858\n",
            "10       ปลอม  0.037294\n",
            "11      ไม่มี  0.033289\n",
            "12     แย่มาก  0.031837\n",
            "13       ซื้อ  0.029743\n",
            "14        ดาว  0.029365\n",
            "15         ใส  0.029294\n",
            "16         เต  0.027804\n",
            "17          ์  0.027295\n",
            "18         อร  0.027175\n",
            "19        เคา  0.026271\n",
            "20        แรง  0.026261\n",
            "21         ดู  0.025764\n",
            "22     สินค้า  0.023698\n",
            "23       อ่อน  0.023369\n",
            "24   ไม่แน่ใจ  0.023351,        feature     score\n",
            "0          ขวด  0.285492\n",
            "1         เก่า  0.196450\n",
            "2   รอยขีดข่วน  0.188839\n",
            "3          รอย  0.135593\n",
            "4     ดูเหมือน  0.125432\n",
            "5       เหมือน  0.112858\n",
            "6       นิดนึง  0.099037\n",
            "7         ถลอก  0.061447\n",
            "8     หักคะแนน  0.060731\n",
            "9          ลาย  0.055083\n",
            "10      reused  0.052366\n",
            "11          ดู  0.050507\n",
            "12      แย่มาก  0.050069\n",
            "13         คัน  0.049810\n",
            "14       บรรจุ  0.045385\n",
            "15         เปน  0.042641\n",
            "16         ค้า  0.041826\n",
            "17        เต็ม  0.038642\n",
            "18          ทา  0.036445\n",
            "19        สภาพ  0.036255\n",
            "20      สินค้า  0.032883\n",
            "21     ใช้แล้ว  0.029193\n",
            "22        หน้า  0.028881\n",
            "23       ละลาย  0.026892\n",
            "24      เปื่อย  0.026892]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26sui_QPpSn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "41b635f1-f171-4b6f-f9b1-6972a7d960f7"
      },
      "source": [
        "# deepcut\n",
        "show_classfeats(\n",
        "    df=train_df,\n",
        "    vectorizer=TfidfVectorizer,\n",
        "    analyzer=process_text,\n",
        "    score_name=\"tf-idf\",\n",
        "    text_col='text',\n",
        "    class_col='label'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    feature     score\n",
            "0       แย่  0.113201\n",
            "1        ดี  0.090645\n",
            "2    สินค้า  0.065791\n",
            "3     กล่อง  0.058257\n",
            "4     พอใช้  0.044411\n",
            "5    บริการ  0.042601\n",
            "6       ขวด  0.039956\n",
            "7       ห่อ  0.038048\n",
            "8      ร้าน  0.037076\n",
            "9       ค้า  0.034071\n",
            "10   คุณภาพ  0.032608\n",
            "11  คุ้มค่า  0.032282\n",
            "12     เก่า  0.028157\n",
            "13     แพ็ค  0.024292\n",
            "14     สั่ง  0.024005\n",
            "15       ไว  0.022791\n",
            "16      ตอบ  0.021842\n",
            "17     โอเค  0.019120\n",
            "18      แชท  0.018931\n",
            "19    ทดลอง  0.018918\n",
            "20  ขีดข่วน  0.017123\n",
            "21      รอย  0.016933\n",
            "22     เช็ค  0.016683\n",
            "23      บุบ  0.016236\n",
            "24     2017  0.016193,    feature     score\n",
            "0    กลิ่น  0.092257\n",
            "1   เหมือน  0.078598\n",
            "2      ขวด  0.076909\n",
            "3    เนื้อ  0.066800\n",
            "4   สินค้า  0.059738\n",
            "5     ซื้อ  0.051740\n",
            "6       ดู  0.046287\n",
            "7     แปลก  0.041672\n",
            "8      รอย  0.040543\n",
            "9    แน่ใจ  0.034675\n",
            "10    เหลว  0.034526\n",
            "11      สี  0.034498\n",
            "12      ดี  0.034336\n",
            "13    ปลอม  0.032851\n",
            "14  คุณภาพ  0.031028\n",
            "15    ปกติ  0.030462\n",
            "16     ตัว  0.029907\n",
            "17  เซรั่ม  0.028638\n",
            "18     จาง  0.023420\n",
            "19      ml  0.023166\n",
            "20    โค้ด  0.023147\n",
            "21    สั่ง  0.022487\n",
            "22     ดาว  0.022122\n",
            "23     ลอง  0.021972\n",
            "24   เปล่า  0.021355]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctsbF5hZUbjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "a541f0b3-6b0c-4a0b-d161-8868dbe07bba"
      },
      "source": [
        "# attacut\n",
        "show_classfeats(\n",
        "    df=train_df,\n",
        "    vectorizer=TfidfVectorizer,\n",
        "    analyzer=process_text,\n",
        "    score_name=\"tf-idf\",\n",
        "    text_col='text',\n",
        "    class_col='label'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[        feature     score\n",
            "0     การจัดส่ง  0.081370\n",
            "1         พอใช้  0.062821\n",
            "2           แย่  0.061858\n",
            "3            ดี  0.058642\n",
            "4        สินค้า  0.057104\n",
            "5         กล่อง  0.054452\n",
            "6        แย่มาก  0.041082\n",
            "7           ขวด  0.040676\n",
            "8           ห่อ  0.036990\n",
            "9        คุณภาพ  0.032263\n",
            "10        ดีมาก  0.030455\n",
            "11         แพ็ค  0.030402\n",
            "12      ร้านค้า  0.030189\n",
            "13    ให้บริการ  0.028891\n",
            "14  ความคุ้มค่า  0.028814\n",
            "15         เก่า  0.028385\n",
            "16        ไม่มี  0.027937\n",
            "17         โอเค  0.027226\n",
            "18   รอยขีดข่วน  0.025957\n",
            "19           ไว  0.023370\n",
            "20          แชท  0.021856\n",
            "21          ตอบ  0.021641\n",
            "22         นะคะ  0.019172\n",
            "23           แพ  0.017531\n",
            "24            ค  0.017531,       feature     score\n",
            "0       กลิ่น  0.079389\n",
            "1         ขวด  0.074387\n",
            "2      เหมือน  0.063626\n",
            "3       เนื้อ  0.060660\n",
            "4      ของแท้  0.050591\n",
            "5      สินค้า  0.049196\n",
            "6        ซื้อ  0.043602\n",
            "7          เซ  0.040433\n",
            "8        รั่ม  0.039284\n",
            "9        เหลว  0.038385\n",
            "10       แปลก  0.037903\n",
            "11         ดู  0.033589\n",
            "12   ไม่แน่ใจ  0.033105\n",
            "13        รอย  0.029457\n",
            "14         สี  0.027688\n",
            "15     คุณภาพ  0.027437\n",
            "16       โค้ด  0.026341\n",
            "17       ปลอม  0.024677\n",
            "18  เหมือนกัน  0.023575\n",
            "19        ลอง  0.023410\n",
            "20         ml  0.022532\n",
            "21        ดาว  0.021634\n",
            "22      กล่อง  0.021181\n",
            "23         เต  0.019747\n",
            "24         อร  0.019747]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFAaH7iTXK77"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}